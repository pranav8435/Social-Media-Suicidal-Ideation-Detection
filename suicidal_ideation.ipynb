{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd99d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce798ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original downloaded dataset, can download following the link in my paper\n",
    "df = pd.read_csv(\"suicide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205131fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up dataset and get rid of values that can't be processed\n",
    "df = df.loc[df.Post.apply(lambda x: not isinstance(x, (float, int)))]\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "df = df.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "df = df.replace(to_replace=r'\\d', value='', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d218cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering for Post Length, and also tokenizing the original posts so we can work with it later\n",
    "lengthList = []\n",
    "for x in df[\"Post\"]:\n",
    "    lengthList.append(len(x)) \n",
    "df[\"Post_length\"] = lengthList\n",
    "df['Tokenized'] = df['Post'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee86cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of stopwords and apply that in the tokenized column \n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Tokenized'] = df['Tokenized'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb33582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pranavsomani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pranavsomani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Standard process for lemmatizing Tokens and getting the lemmas/root form of each word. This further reduces noise \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    def get_wordnet_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "# apply lemmatization function to column of dataframe\n",
    "df['lemmatized_messages'] = df['Tokenized'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d924b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use GloVe embeddings, we need to import a corpus with a certain dimensionality. \n",
    "#I chose 100 as my dimensionality it isn't too costly while still preserves a lot of information\n",
    "\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    embedding_dim = len(coefs)\n",
    "    return embeddings_index, embedding_dim\n",
    "\n",
    "#make sure to download this file online by searching up GloVe embeddings...couldn't attach this in my submission due to size\n",
    "glove_embeddings, embedding_dim = load_glove_embeddings('/Users/pranavsomani/Downloads/glove.6B/glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f862eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates an embedding for every word in a post \n",
    "def sentence_to_embedding(sentence, embeddings_index, embedding_dim):\n",
    "    embeddings = []\n",
    "    for word in sentence:\n",
    "        if word in embeddings_index:\n",
    "            embeddings.append(embeddings_index[word])\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "#Applies the embedding on the lemmatized messages column\n",
    "df['sentence_embedding'] = df['lemmatized_messages'].apply(lambda x: sentence_to_embedding(x, glove_embeddings, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee9a4100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregates all the embedded values for a sentence so machine learning models can be supplied this feature\n",
    "def reshape(vector):\n",
    "    y = np.array(vector)\n",
    "    mean = np.mean(vector)\n",
    "    mean = np.array(mean)\n",
    "    return mean.reshape(-1, 1)\n",
    "\n",
    "df[\"sentence_embedding\"] = df['sentence_embedding'].apply(lambda x: reshape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32b21fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#Further feature engineering: implementing a highly-used sentiment analysis model on each post in the dataset\n",
    "#This will serve as a feature to see how strongly negative/positive a post is\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", truncation=True)\n",
    "\n",
    "def sentimentAnalysis(data):    \n",
    "    if type(data) != str:\n",
    "        return 0\n",
    "    \n",
    "    x = sentiment_pipeline(data)\n",
    "    return x\n",
    "\n",
    "#Implementing the model on each post in dataframe....takes very long so run at your own risk\n",
    "df['sentimentAnalysis2'] = df['Post'].apply(lambda x: sentimentAnalysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e5698de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the output of the sentimentAnalysis column and changes the format of it to simply be an integer\n",
    "def sentimentNum(row):\n",
    "    res = row\n",
    "    if res == 0:\n",
    "        return 0\n",
    "    sentDict = res[0]\n",
    "    if sentDict[\"label\"] == 'NEGATIVE':\n",
    "        return sentDict[\"score\"] * -1\n",
    "    elif sentDict[\"label\"] == 'POSITIVE':\n",
    "        return sentDict[\"score\"]\n",
    "    \n",
    "df['SentAnalysis'] = df['sentimentAnalysis2'].apply(lambda x: sentimentNum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fbc87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('badwords.txt', 'r') as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "\n",
    "def badWords(badwordList, txt):\n",
    "    counter = 0\n",
    "    txt = txt.lower()\n",
    "    txt = txt.split()\n",
    "    for string in txt:\n",
    "        if string in badwordList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "df['badWordCount'] = df['Post'].apply(lambda x: badWords(lines, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e17c1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>Post_length</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>lemmatized_messages</th>\n",
       "      <th>sentence_embedding</th>\n",
       "      <th>sentimentAnalysis2</th>\n",
       "      <th>SentAnalysis</th>\n",
       "      <th>badWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im looking for a girl ive met at the polish ai...</td>\n",
       "      <td>ive tried polish spotted pages but i guess she...</td>\n",
       "      <td>nonsuicidal</td>\n",
       "      <td>1022</td>\n",
       "      <td>[ive, tried, polish, spotted, pages, guess, is...</td>\n",
       "      <td>[ive, try, polish, spot, page, guess, isnt, kn...</td>\n",
       "      <td>[[-0.020897085]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.695042848587...</td>\n",
       "      <td>-0.695043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i got a guy kicked off a domestic flight today</td>\n",
       "      <td>i was in a forward row as we were boarding for...</td>\n",
       "      <td>nonsuicidal</td>\n",
       "      <td>919</td>\n",
       "      <td>[forward, row, boarding, hour, domestic, fligh...</td>\n",
       "      <td>[forward, row, boarding, hour, domestic, fligh...</td>\n",
       "      <td>[[-0.015430211]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997833907604...</td>\n",
       "      <td>-0.997834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my youngest got in school suspension im so proud</td>\n",
       "      <td>so according to witness testimonies a boy grab...</td>\n",
       "      <td>nonsuicidal</td>\n",
       "      <td>355</td>\n",
       "      <td>[according, witness, testimonies, boy, grabbed...</td>\n",
       "      <td>[accord, witness, testimony, boy, grabbed, say...</td>\n",
       "      <td>[[-0.008069504]]</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.938526093959...</td>\n",
       "      <td>0.938526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im a cashier who switched from sirmam to my de...</td>\n",
       "      <td>so as said in the title im a cashier well that...</td>\n",
       "      <td>nonsuicidal</td>\n",
       "      <td>2434</td>\n",
       "      <td>[said, title, im, cashier, well, thats, part, ...</td>\n",
       "      <td>[say, title, im, cashier, well, thats, part, j...</td>\n",
       "      <td>[[-0.023101069]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.986810863018...</td>\n",
       "      <td>-0.986811</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my whole class complimented me and didnt reali...</td>\n",
       "      <td>today in class we were doing this activity cal...</td>\n",
       "      <td>nonsuicidal</td>\n",
       "      <td>869</td>\n",
       "      <td>[today, class, activity, called, someone, basi...</td>\n",
       "      <td>[today, class, activity, call, someone, basica...</td>\n",
       "      <td>[[-0.033342287]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.908012390136...</td>\n",
       "      <td>-0.908012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15472</th>\n",
       "      <td>once i die i can finally be at rest</td>\n",
       "      <td>my past actions will no longer haunt me hopefu...</td>\n",
       "      <td>suicidal</td>\n",
       "      <td>97</td>\n",
       "      <td>[past, actions, longer, haunt, hopefully, peop...</td>\n",
       "      <td>[past, action, longer, haunt, hopefully, peopl...</td>\n",
       "      <td>[[-0.025204448]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.995922803878...</td>\n",
       "      <td>-0.995923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15473</th>\n",
       "      <td>i just want to stop</td>\n",
       "      <td>i just want to stop living ive lost everything...</td>\n",
       "      <td>suicidal</td>\n",
       "      <td>580</td>\n",
       "      <td>[want, stop, living, ive, lost, everything, im...</td>\n",
       "      <td>[want, stop, living, ive, lose, everything, im...</td>\n",
       "      <td>[[-0.016652748]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997233331203...</td>\n",
       "      <td>-0.997233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>im still alive</td>\n",
       "      <td>why the fuck am i still alive why wont i just ...</td>\n",
       "      <td>suicidal</td>\n",
       "      <td>591</td>\n",
       "      <td>[fuck, still, alive, wont, fucking, kill, alre...</td>\n",
       "      <td>[fuck, still, alive, wont, fuck, kill, already...</td>\n",
       "      <td>[[-0.015870886]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.998081088066...</td>\n",
       "      <td>-0.998081</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15475</th>\n",
       "      <td>im lonely but i cant stand people</td>\n",
       "      <td>its a lot better online but irl i cant stand p...</td>\n",
       "      <td>suicidal</td>\n",
       "      <td>1260</td>\n",
       "      <td>[lot, better, online, irl, cant, stand, people...</td>\n",
       "      <td>[lot, well, online, irl, cant, stand, people, ...</td>\n",
       "      <td>[[-0.021809079]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.980329394340...</td>\n",
       "      <td>-0.980329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15476</th>\n",
       "      <td>i dont even recognize myself</td>\n",
       "      <td>so whats the point of saving someone i dont ev...</td>\n",
       "      <td>suicidal</td>\n",
       "      <td>855</td>\n",
       "      <td>[whats, point, saving, someone, dont, even, kn...</td>\n",
       "      <td>[whats, point, save, someone, dont, even, know...</td>\n",
       "      <td>[[-0.016123408]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.998644649982...</td>\n",
       "      <td>-0.998645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14243 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      im looking for a girl ive met at the polish ai...   \n",
       "1         i got a guy kicked off a domestic flight today   \n",
       "2       my youngest got in school suspension im so proud   \n",
       "3      im a cashier who switched from sirmam to my de...   \n",
       "4      my whole class complimented me and didnt reali...   \n",
       "...                                                  ...   \n",
       "15472                once i die i can finally be at rest   \n",
       "15473                                i just want to stop   \n",
       "15474                                     im still alive   \n",
       "15475                  im lonely but i cant stand people   \n",
       "15476                       i dont even recognize myself   \n",
       "\n",
       "                                                    Post        Label  \\\n",
       "0      ive tried polish spotted pages but i guess she...  nonsuicidal   \n",
       "1      i was in a forward row as we were boarding for...  nonsuicidal   \n",
       "2      so according to witness testimonies a boy grab...  nonsuicidal   \n",
       "3      so as said in the title im a cashier well that...  nonsuicidal   \n",
       "4      today in class we were doing this activity cal...  nonsuicidal   \n",
       "...                                                  ...          ...   \n",
       "15472  my past actions will no longer haunt me hopefu...     suicidal   \n",
       "15473  i just want to stop living ive lost everything...     suicidal   \n",
       "15474  why the fuck am i still alive why wont i just ...     suicidal   \n",
       "15475  its a lot better online but irl i cant stand p...     suicidal   \n",
       "15476  so whats the point of saving someone i dont ev...     suicidal   \n",
       "\n",
       "       Post_length                                          Tokenized  \\\n",
       "0             1022  [ive, tried, polish, spotted, pages, guess, is...   \n",
       "1              919  [forward, row, boarding, hour, domestic, fligh...   \n",
       "2              355  [according, witness, testimonies, boy, grabbed...   \n",
       "3             2434  [said, title, im, cashier, well, thats, part, ...   \n",
       "4              869  [today, class, activity, called, someone, basi...   \n",
       "...            ...                                                ...   \n",
       "15472           97  [past, actions, longer, haunt, hopefully, peop...   \n",
       "15473          580  [want, stop, living, ive, lost, everything, im...   \n",
       "15474          591  [fuck, still, alive, wont, fucking, kill, alre...   \n",
       "15475         1260  [lot, better, online, irl, cant, stand, people...   \n",
       "15476          855  [whats, point, saving, someone, dont, even, kn...   \n",
       "\n",
       "                                     lemmatized_messages sentence_embedding  \\\n",
       "0      [ive, try, polish, spot, page, guess, isnt, kn...   [[-0.020897085]]   \n",
       "1      [forward, row, boarding, hour, domestic, fligh...   [[-0.015430211]]   \n",
       "2      [accord, witness, testimony, boy, grabbed, say...   [[-0.008069504]]   \n",
       "3      [say, title, im, cashier, well, thats, part, j...   [[-0.023101069]]   \n",
       "4      [today, class, activity, call, someone, basica...   [[-0.033342287]]   \n",
       "...                                                  ...                ...   \n",
       "15472  [past, action, longer, haunt, hopefully, peopl...   [[-0.025204448]]   \n",
       "15473  [want, stop, living, ive, lose, everything, im...   [[-0.016652748]]   \n",
       "15474  [fuck, still, alive, wont, fuck, kill, already...   [[-0.015870886]]   \n",
       "15475  [lot, well, online, irl, cant, stand, people, ...   [[-0.021809079]]   \n",
       "15476  [whats, point, save, someone, dont, even, know...   [[-0.016123408]]   \n",
       "\n",
       "                                      sentimentAnalysis2  SentAnalysis  \\\n",
       "0      [{'label': 'NEGATIVE', 'score': 0.695042848587...     -0.695043   \n",
       "1      [{'label': 'NEGATIVE', 'score': 0.997833907604...     -0.997834   \n",
       "2      [{'label': 'POSITIVE', 'score': 0.938526093959...      0.938526   \n",
       "3      [{'label': 'NEGATIVE', 'score': 0.986810863018...     -0.986811   \n",
       "4      [{'label': 'NEGATIVE', 'score': 0.908012390136...     -0.908012   \n",
       "...                                                  ...           ...   \n",
       "15472  [{'label': 'NEGATIVE', 'score': 0.995922803878...     -0.995923   \n",
       "15473  [{'label': 'NEGATIVE', 'score': 0.997233331203...     -0.997233   \n",
       "15474  [{'label': 'NEGATIVE', 'score': 0.998081088066...     -0.998081   \n",
       "15475  [{'label': 'NEGATIVE', 'score': 0.980329394340...     -0.980329   \n",
       "15476  [{'label': 'NEGATIVE', 'score': 0.998644649982...     -0.998645   \n",
       "\n",
       "       badWordCount  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "15472             0  \n",
       "15473             0  \n",
       "15474             4  \n",
       "15475             1  \n",
       "15476             0  \n",
       "\n",
       "[14243 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a143098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes the y labels of the dataset binary\n",
    "label_mapping = {'suicidal': 1, 'nonsuicidal': 0}\n",
    "df['Label'] = df['Label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "160ea602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Made different datasets of both classes to be able to find any significant differences in post length and sentiment\n",
    "suicidal_df = df[df[\"Label\"] == 1]\n",
    "nonsuicidal_df = df[df[\"Label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962c3936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        Label   Post_length  SentAnalysis  badWordCount\n",
       " count  6806.0   6806.000000   6806.000000   6806.000000\n",
       " mean      1.0    817.997943     -0.843184      0.904790\n",
       " std       0.0   1011.940750      0.503856     10.329561\n",
       " min       1.0      0.000000     -0.999812      0.000000\n",
       " 25%       1.0    255.000000     -0.999182      0.000000\n",
       " 50%       1.0    533.000000     -0.997978      0.000000\n",
       " 75%       1.0   1020.000000     -0.992421      1.000000\n",
       " max       1.0  18413.000000      0.999878    837.000000,\n",
       "         Label   Post_length  SentAnalysis  badWordCount\n",
       " count  7436.0   7436.000000   7436.000000   7436.000000\n",
       " mean      0.0    626.681818     -0.029710      0.157746\n",
       " std       0.0    674.648277      0.959588      0.601272\n",
       " min       0.0      0.000000     -0.999816      0.000000\n",
       " 25%       0.0    218.000000     -0.993674      0.000000\n",
       " 50%       0.0    422.000000     -0.650665      0.000000\n",
       " 75%       0.0    808.000000      0.994332      0.000000\n",
       " max       0.0  13665.000000      0.999891     14.000000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicidal_df.describe(), nonsuicidal_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f69de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Bootstrapping algorithm I made in pset5 to see whether or not there is a significant difference in the length of posts between those struggling with suicidal ideation and those who aren't\n",
    "#Suicide Class size\n",
    "N = 6806.000000\n",
    "#Non-Suicide Class size\n",
    "M = 7436.000000\n",
    "\n",
    "average2 = 0.904790\n",
    "average1 = 0.157746\n",
    "score = df[\"badWordCount\"].values.tolist()\n",
    "\n",
    "observedDiff = abs(average2-average1)\n",
    "count = 0 \n",
    "\n",
    "for i in range(20000):\n",
    "    act1_resample = np.random.choice(score, int(N), replace= True)\n",
    "    act2_resample = np.random.choice(score, int(M), replace= True)\n",
    "    \n",
    "    npmean1 = np.mean(act1_resample)\n",
    "    npmean2 = np.mean(act2_resample)\n",
    "    \n",
    "    diff = abs(npmean2 - npmean1)\n",
    "    if diff >=  observedDiff:\n",
    "        count+=1\n",
    "\n",
    "print(count)\n",
    "pvalue = (count/20000)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed9cb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Bootstrapping algorithm I made in pset5 to see whether or not there is a significant difference in the length of posts between those struggling with suicidal ideation and those who aren't\n",
    "\n",
    "#Suicide Class size\n",
    "N = 6806.000000\n",
    "#Non-Suicide Class size\n",
    "M = 7436.000000\n",
    "\n",
    "average2 = 817.997943\n",
    "average1 = 626.681818\n",
    "score = df[\"Post_length\"].values.tolist()\n",
    "\n",
    "observedDiff = abs(average2-average1)\n",
    "count = 0 \n",
    "\n",
    "for i in range(10000):\n",
    "    act1_resample = np.random.choice(score, int(N), replace= True)\n",
    "    act2_resample = np.random.choice(score, int(M), replace= True)\n",
    "    \n",
    "    npmean1 = np.mean(act1_resample)\n",
    "    npmean2 = np.mean(act2_resample)\n",
    "    \n",
    "    diff = abs(npmean2 - npmean1)\n",
    "    if diff >=  observedDiff:\n",
    "        count+=1\n",
    "\n",
    "print(count)\n",
    "pvalue = (count/10000)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd58b30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Bootstrapping algorithm I made in pset5 to see whether or not there is a significant difference in the sentiment of posts between those struggling with suicidal ideation and those who aren't\n",
    "\n",
    "#Suicide Class size\n",
    "N = 6806.000000\n",
    "#Non-Suicide Class size\n",
    "M = 7436.000000\n",
    "\n",
    "average2 = -0.844678\n",
    "average1 = -0.030158\n",
    "score = df[\"SentAnalysis\"].values.tolist()\n",
    "\n",
    "\n",
    "observedDiff = abs(average2-average1)\n",
    "count = 0 \n",
    "\n",
    "for i in range(10000):\n",
    "    act1_resample = np.random.choice(score, int(N), replace= True)\n",
    "    act2_resample = np.random.choice(score, int(M), replace= True)\n",
    "    \n",
    "    npmean1 = np.mean(act1_resample)\n",
    "    npmean2 = np.mean(act2_resample)\n",
    "    \n",
    "    diff = abs(npmean2 - npmean1)\n",
    "    if diff >=  observedDiff:\n",
    "        count+=1\n",
    "\n",
    "print(count)      \n",
    "pvalue = count/10000\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "435756fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy RF: 0.8138330553848855\n",
      "Test Accuracy RF: 0.7581607581607581\n"
     ]
    }
   ],
   "source": [
    "#Training RF classifier on my features and output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "X = df[[\"sentence_embedding\", \"Post_length\", \"SentAnalysis\", \"badWordCount\"]]\n",
    "X = np.array(X)\n",
    "y = np.array(df['Label'])\n",
    "\n",
    "#80% training and 20% testing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Specified depth to prevent overfitting\n",
    "rf = RandomForestClassifier(max_depth=10)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_accuracy_rf = rf.score(X_train, y_train)\n",
    "test_accuracy_rf = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"Train Accuracy RF:\", train_accuracy_rf)\n",
    "print(\"Test Accuracy RF:\", test_accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5af97598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model to disk\n",
    "filename = 'SI_finalized_model.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))\n",
    "\n",
    "#Can use model without having to train it again\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65d4d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialize dataframe so everything doesn't become strings\n",
    "df.to_pickle('pickle-file.pkl.gz', compression='gzip')\n",
    "df = pd.read_pickle('pickle-file.pkl.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8c59fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Post</th>\n",
       "      <th>Label</th>\n",
       "      <th>Post_length</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>lemmatized_messages</th>\n",
       "      <th>sentence_embedding</th>\n",
       "      <th>sentimentAnalysis2</th>\n",
       "      <th>SentAnalysis</th>\n",
       "      <th>badWordCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im looking for a girl ive met at the polish ai...</td>\n",
       "      <td>ive tried polish spotted pages but i guess she...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1022</td>\n",
       "      <td>[ive, tried, polish, spotted, pages, guess, is...</td>\n",
       "      <td>[ive, try, polish, spot, page, guess, isnt, kn...</td>\n",
       "      <td>[[-0.020897085]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.695042848587...</td>\n",
       "      <td>-0.695043</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i got a guy kicked off a domestic flight today</td>\n",
       "      <td>i was in a forward row as we were boarding for...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>919</td>\n",
       "      <td>[forward, row, boarding, hour, domestic, fligh...</td>\n",
       "      <td>[forward, row, boarding, hour, domestic, fligh...</td>\n",
       "      <td>[[-0.015430211]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997833907604...</td>\n",
       "      <td>-0.997834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my youngest got in school suspension im so proud</td>\n",
       "      <td>so according to witness testimonies a boy grab...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>355</td>\n",
       "      <td>[according, witness, testimonies, boy, grabbed...</td>\n",
       "      <td>[accord, witness, testimony, boy, grabbed, say...</td>\n",
       "      <td>[[-0.008069504]]</td>\n",
       "      <td>[{'label': 'POSITIVE', 'score': 0.938526093959...</td>\n",
       "      <td>0.938526</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im a cashier who switched from sirmam to my de...</td>\n",
       "      <td>so as said in the title im a cashier well that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2434</td>\n",
       "      <td>[said, title, im, cashier, well, thats, part, ...</td>\n",
       "      <td>[say, title, im, cashier, well, thats, part, j...</td>\n",
       "      <td>[[-0.023101069]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.986810863018...</td>\n",
       "      <td>-0.986811</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my whole class complimented me and didnt reali...</td>\n",
       "      <td>today in class we were doing this activity cal...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>869</td>\n",
       "      <td>[today, class, activity, called, someone, basi...</td>\n",
       "      <td>[today, class, activity, call, someone, basica...</td>\n",
       "      <td>[[-0.033342287]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.908012390136...</td>\n",
       "      <td>-0.908012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15472</th>\n",
       "      <td>once i die i can finally be at rest</td>\n",
       "      <td>my past actions will no longer haunt me hopefu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>97</td>\n",
       "      <td>[past, actions, longer, haunt, hopefully, peop...</td>\n",
       "      <td>[past, action, longer, haunt, hopefully, peopl...</td>\n",
       "      <td>[[-0.025204448]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.995922803878...</td>\n",
       "      <td>-0.995923</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15473</th>\n",
       "      <td>i just want to stop</td>\n",
       "      <td>i just want to stop living ive lost everything...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>580</td>\n",
       "      <td>[want, stop, living, ive, lost, everything, im...</td>\n",
       "      <td>[want, stop, living, ive, lose, everything, im...</td>\n",
       "      <td>[[-0.016652748]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.997233331203...</td>\n",
       "      <td>-0.997233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15474</th>\n",
       "      <td>im still alive</td>\n",
       "      <td>why the fuck am i still alive why wont i just ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>591</td>\n",
       "      <td>[fuck, still, alive, wont, fucking, kill, alre...</td>\n",
       "      <td>[fuck, still, alive, wont, fuck, kill, already...</td>\n",
       "      <td>[[-0.015870886]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.998081088066...</td>\n",
       "      <td>-0.998081</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15475</th>\n",
       "      <td>im lonely but i cant stand people</td>\n",
       "      <td>its a lot better online but irl i cant stand p...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1260</td>\n",
       "      <td>[lot, better, online, irl, cant, stand, people...</td>\n",
       "      <td>[lot, well, online, irl, cant, stand, people, ...</td>\n",
       "      <td>[[-0.021809079]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.980329394340...</td>\n",
       "      <td>-0.980329</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15476</th>\n",
       "      <td>i dont even recognize myself</td>\n",
       "      <td>so whats the point of saving someone i dont ev...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>855</td>\n",
       "      <td>[whats, point, saving, someone, dont, even, kn...</td>\n",
       "      <td>[whats, point, save, someone, dont, even, know...</td>\n",
       "      <td>[[-0.016123408]]</td>\n",
       "      <td>[{'label': 'NEGATIVE', 'score': 0.998644649982...</td>\n",
       "      <td>-0.998645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14242 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      im looking for a girl ive met at the polish ai...   \n",
       "1         i got a guy kicked off a domestic flight today   \n",
       "2       my youngest got in school suspension im so proud   \n",
       "3      im a cashier who switched from sirmam to my de...   \n",
       "4      my whole class complimented me and didnt reali...   \n",
       "...                                                  ...   \n",
       "15472                once i die i can finally be at rest   \n",
       "15473                                i just want to stop   \n",
       "15474                                     im still alive   \n",
       "15475                  im lonely but i cant stand people   \n",
       "15476                       i dont even recognize myself   \n",
       "\n",
       "                                                    Post  Label  Post_length  \\\n",
       "0      ive tried polish spotted pages but i guess she...    0.0         1022   \n",
       "1      i was in a forward row as we were boarding for...    0.0          919   \n",
       "2      so according to witness testimonies a boy grab...    0.0          355   \n",
       "3      so as said in the title im a cashier well that...    0.0         2434   \n",
       "4      today in class we were doing this activity cal...    0.0          869   \n",
       "...                                                  ...    ...          ...   \n",
       "15472  my past actions will no longer haunt me hopefu...    1.0           97   \n",
       "15473  i just want to stop living ive lost everything...    1.0          580   \n",
       "15474  why the fuck am i still alive why wont i just ...    1.0          591   \n",
       "15475  its a lot better online but irl i cant stand p...    1.0         1260   \n",
       "15476  so whats the point of saving someone i dont ev...    1.0          855   \n",
       "\n",
       "                                               Tokenized  \\\n",
       "0      [ive, tried, polish, spotted, pages, guess, is...   \n",
       "1      [forward, row, boarding, hour, domestic, fligh...   \n",
       "2      [according, witness, testimonies, boy, grabbed...   \n",
       "3      [said, title, im, cashier, well, thats, part, ...   \n",
       "4      [today, class, activity, called, someone, basi...   \n",
       "...                                                  ...   \n",
       "15472  [past, actions, longer, haunt, hopefully, peop...   \n",
       "15473  [want, stop, living, ive, lost, everything, im...   \n",
       "15474  [fuck, still, alive, wont, fucking, kill, alre...   \n",
       "15475  [lot, better, online, irl, cant, stand, people...   \n",
       "15476  [whats, point, saving, someone, dont, even, kn...   \n",
       "\n",
       "                                     lemmatized_messages sentence_embedding  \\\n",
       "0      [ive, try, polish, spot, page, guess, isnt, kn...   [[-0.020897085]]   \n",
       "1      [forward, row, boarding, hour, domestic, fligh...   [[-0.015430211]]   \n",
       "2      [accord, witness, testimony, boy, grabbed, say...   [[-0.008069504]]   \n",
       "3      [say, title, im, cashier, well, thats, part, j...   [[-0.023101069]]   \n",
       "4      [today, class, activity, call, someone, basica...   [[-0.033342287]]   \n",
       "...                                                  ...                ...   \n",
       "15472  [past, action, longer, haunt, hopefully, peopl...   [[-0.025204448]]   \n",
       "15473  [want, stop, living, ive, lose, everything, im...   [[-0.016652748]]   \n",
       "15474  [fuck, still, alive, wont, fuck, kill, already...   [[-0.015870886]]   \n",
       "15475  [lot, well, online, irl, cant, stand, people, ...   [[-0.021809079]]   \n",
       "15476  [whats, point, save, someone, dont, even, know...   [[-0.016123408]]   \n",
       "\n",
       "                                      sentimentAnalysis2  SentAnalysis  \\\n",
       "0      [{'label': 'NEGATIVE', 'score': 0.695042848587...     -0.695043   \n",
       "1      [{'label': 'NEGATIVE', 'score': 0.997833907604...     -0.997834   \n",
       "2      [{'label': 'POSITIVE', 'score': 0.938526093959...      0.938526   \n",
       "3      [{'label': 'NEGATIVE', 'score': 0.986810863018...     -0.986811   \n",
       "4      [{'label': 'NEGATIVE', 'score': 0.908012390136...     -0.908012   \n",
       "...                                                  ...           ...   \n",
       "15472  [{'label': 'NEGATIVE', 'score': 0.995922803878...     -0.995923   \n",
       "15473  [{'label': 'NEGATIVE', 'score': 0.997233331203...     -0.997233   \n",
       "15474  [{'label': 'NEGATIVE', 'score': 0.998081088066...     -0.998081   \n",
       "15475  [{'label': 'NEGATIVE', 'score': 0.980329394340...     -0.980329   \n",
       "15476  [{'label': 'NEGATIVE', 'score': 0.998644649982...     -0.998645   \n",
       "\n",
       "       badWordCount  \n",
       "0                 1  \n",
       "1                 0  \n",
       "2                 1  \n",
       "3                 0  \n",
       "4                 0  \n",
       "...             ...  \n",
       "15472             0  \n",
       "15473             0  \n",
       "15474             4  \n",
       "15475             1  \n",
       "15476             0  \n",
       "\n",
       "[14242 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d5a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5928649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
