**CS109 Challenge Project: Developing a Tool to Recognize Suicidal Ideation**

Introduction: According to the CDC, in 2019, more than 12 million Americans contemplated suicide. Among them, 1.4 million attempted to end their lives, and tragically, 47,500 individuals died by suicide. This issue has become so pervasive that the CDC has stated that suicide is the eleventh leading cause of death in the United States. After COVID-19, we’ve seen a sharp increase in social media usage—usage that has been linked to loneliness and depression, key risk factors that contribute to suicidal ideation and suicide. While social media can be incredibly detrimental to one’s mental health, it is also clear that social media is integral to modern society. However, what if there was a way to harness social media to combat mental health struggles rather than perpetuate them? What if the very platforms that frequently recommend content damaging to one’s psyche recognized the mental state of a user, guiding them towards the proper help they need if they need it? 

Project Goal: The goal of this project is to develop a tool capable of recognizing suicidal ideation through the online activity of a user, specifically, from what they post online. 

Methodology: To develop such a tool, I needed a dataset differentiating those with suicidal ideation and those without. Such a dataset was acquired from Mendeley Data. The dataset comprises Reddit posts from popular subreddits such as r/CasualConversation, r/BenignExistence, and r/CongratsLikeImFive, depicting non-suicidal text. Posts that were part of the class of suicidal ideation were from r/SuicideWatch, a subreddit composed of those struggling with suicidal intent and mental health issues as a whole. This dataset was composed of 15,477 records and posed a binary classification task, differentiating between non-suicidal and suicidal text.

This dataset was purely composed of textual data, however, to train a machine learning model, I needed to represent these words as numerical values. This posed a significant task: how do I go about preprocessing such data?

After researching online, I found standard preprocessing NLP techniques to remove noise from textual data, including tokenization and lemmatization. After having reduced noise from the dataset, I then needed to represent these representations as vectors for a machine learning model to be able to understand. Though I haven’t taken CS224N, I followed a link on the CS224N website to understand how I should go about doing so. I ended up using GloVe, a way to represent text through matrix factorization (something a bit beyond the scope of this class). However, to briefly describe what GloVe does, it essentially constructs a co-occurrence matrix from a given corpus to capture how often words appear together, then uses matrix factorization to reduce this data into a lower-dimensional embedding space. The result is a set of word vectors where similar words have similar representations. This set of words then needed to be aggregated into a singular value that would be understood by a machine-learning model, leading me to take the average of all the numerical representations in the word vector. 

However, simply having words be represented as numerical representations isn’t enough—it’s common when building machine learning algorithms to take in various features. This was seen in problem set 6 as well, where all the datasets used to train the Naive Bayes and Logistic Regression classifiers had multiple features, providing more information for the classification models to work with. To extract more features from the purely textual dataset, I decided to explore if the length of posts and sentiment of those posts would be drastically different for non-suicidal and suicidal text. After having run a bootstrapping algorithm that we designed in problem set 5, I found there to be a significant difference between the length of posts of those who struggled with suicidal ideation and those who didn’t (p-value \< \~0.0001). Similarly, there was a significant difference between the sentiment of the posts written by those who had suicidal thoughts compared to those who didn’t (p-value \< \~0.0001). This indicated that these features may be useful for the machine learning model to use, as there are large differences in the length and sentiment of posts written by those struggling with suicidal ideation compared to those who aren’t.

After having done all this feature extraction, it was time to design my classifier. While in class, we learned about naive Bayes and logistic regression, I found those models do not provide me with the best performance for the data I was dealing with. However, after a bit of research, I found the random forest algorithm to be well-suited for my dataset (in comparison to the other two models). While the random forest algorithm wasn’t covered in class, it leverages topics we discussed in class, as follows.

Intuition Behind the Random Forest Algorithm: A Random Forest is an algorithm that constructs a multitude of decision trees at training and outputs the class/label that is the mode of the classes/labels. In the algorithm, you specify the number of decision trees you would want, and each tree in the Random Forest generates a bootstrap sample of the training data (with replacement). Then, for each bootstrap sample, a decision tree is constructed. Each level of the decision tree selects a random subset of features to be used, and this process recursively keeps occurring until the decision tree is grown to the “stopping criterion” (such as the depth of the tree, which is specified in the algorithm’s implementation). After the trees have been made, each tree "votes" for a class, and the class with the most votes is assigned as the predicted class.  This can be represented by the function below:

<data:image/png;base64,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>

Decision trees on their own are “weak learners,” however combining them with various other trees reduces the variability of the prediction of a single tree. This principle was also learned in class: the more trials we have of something, the lower the variance, as we become more certain about our outcome. Furthermore, the randomness induced by bootstrapping and random feature selection ensures that each decision tree in the ensemble is trained on a slightly different subset of the data and considers a different subset of features. This leads to the individual trees from being decorrelated, which prevents them from making the same errors.

In the case of my random forest model, I had 100 independent and identically distributed decision trees. This meant that each tree had the same level of bias, but due to the high number of trees being combined, the variance of all the trees combined decreased compared to an individual tree (more trials lead to less variance). Furthermore, during each decision tree’s creation, the tree was bootstrapping from my original dataset and randomly selected a size n subset of features from my dataset, where *n* refers to the number of features in my dataset (the reason why it is n is because that is what the inventors of the model recommended for classification tasks). This process keeps occurring, leading the decision tree to grow larger until it reaches a stop criterion.

One important note to mention is that the decision trees themselves are growing and repeating the bootstrapping method that I described earlier, they are continuously making splits based on the Gini index/Gini impurity during the model’s training phase. While Gini impurity goes beyond the scope of the class, it is important to random forest models as it generates the best split for each decision tree by analyzing how often a randomly chosen element from the dataset would be incorrectly labeled if it was randomly labeled according to the distribution of labels present in the tree’s current node. Essentially, at every node before the tree becomes bigger, the decision tree algorithm goes through all possible splits in the dataset and selects the split that results in the lowest Gini index. The Gini index formula that applied to my model can be represented by the function below, where *p* and (1-*p*) represent the probability of an item being in the suicidal ideation class or the non-suicidal ideation class:

<data:image/png;base64,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>

After the splits of the tree reach the specified maximum depth of the tree, the final nodes of the tree, (leaf nodes), contain the predictions. Each leaf node represents a subset of the data that falls into a specific class. The majority class within the leaf node is then assigned as the prediction for any new data points that reach that leaf during prediction (this is basically a counting problem: which class has a higher frequency by the time we reach the last split of the tree). This entire process occurred within my random forest model, where I specified the maximum depth of each decision tree to be 10 levels (usually recommended between 5 and 15 from what I saw online). Each tree recursively split according to the Gini index criterion while bootstrapping data and selecting random features. This process kept occurring until each tree reached its maximum depth. All these decision trees working together led my random forest model to have a train and test accuracy of 0.7867 and 0.7501 respectively.

Discussion and Impact: It’s no secret that social media has led to declining mental health, however, what if those platforms implemented such an algorithm to track the mental health of their users, recommend them beneficial resources and content while also employing mechanisms to discretely suggest a struggling user’s profile to loved ones who may have not reached out to them for an extended period of time. I believe that the social media platforms that design mental-health-based tools for their users would not only dramatically improve the lives of others, but also help them increase user satisfaction and retention with their products. This would recommend a systemic change from social media platforms that may not be realized fast enough, which is why I intend to currently work on my own solution. As of right now, I am extending this project by working on a journaling app that tracks users’ mental health through NLP-based machine-learning models and provides users’ therapists key information about their patient's mental health. This includes tracking user emotions over time and recognizing signs of suicidal ideation, anxiety, and depression, allowing therapists to further personalize their care. Additionally, I also recognize that my current machine learning model can be improved through more feature engineering and also through the implementation of deep learning models.

Conclusion: Ultimately, suicide isn’t a problem that will magically disappear through the use of machine learning; however, I am a firm believer that if used correctly, we can recognize early signs that a person has been thinking about taking their own life. Algorithms like the one I developed are a step toward the right direction—a direction in which those struggling with their thoughts don’t end up acting on them. 

**References:**  
Background Information: [https://www.cdc.gov/suicide/facts/index.html](https://www.cdc.gov/suicide/facts/index.html)  
Dataset found here: [https://data.mendeley.com/datasets/z8s6w86tr3/1](https://data.mendeley.com/datasets/z8s6w86tr3/1)  
Information about GloVe: [https://vinija.ai/nlp/word-vectors/\#overview-2](https://vinija.ai/nlp/word-vectors/#overview-2)  
Pictures and Information about Random Forest: [https://www.cs.cmu.edu/\~qyj/papersA08/11-rfbook.pdf](https://www.cs.cmu.edu/~qyj/papersA08/11-rfbook.pdf)**  
[https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf**](https://www.math.mcgill.ca/yyang/resources/doc/randomforest.pdf)
